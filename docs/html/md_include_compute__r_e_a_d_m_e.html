<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>MagmaDNN: #### Adding New Operations</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">MagmaDNN
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">c++NeuralNetworkFramework</div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('md_include_compute__r_e_a_d_m_e.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">#### Adding New Operations </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The skeleton of the operation superclass can be found at <a href="/include/compute/operation.h">include/compute/operation.h</a>. There are several things that must be implemented in order to have a working operation. Below the process is layed out for the example operation <code>sub</code>.</p>
<ol type="1">
<li>Create a new folder in <code>include/compute</code> and <code>src/compute</code> named <code>sub</code>.</li>
<li>Inside the <code>include/compute/sub</code> folder create <code>subop.h</code> and in <code>src/compute/sub</code> create <code>subop.cpp</code>.</li>
<li>Define the <code>SubOp</code> class, which extends <code>Operation</code>, in the header file. Note that the class must be in the namespace <code>magmadnn::op</code>. The method <code>foo</code> should also be defined here, which returns a new operation class <code>SubOp</code>.</li>
<li>Implement <code>SubOp</code> in <code>subop.cpp</code>. The new operation must implement the constructor, <em>eval, _grad, and to_string methods. <code>_eval</code> should return the evaluated tensor up to this point in the tree. (Note, you are allowed to create helper files within the folder <code>sub/</code>, but their methods must live within the namespace <code>magmadnn::internal</code>). <code>_grad</code> is given the consumer operation, variable that gradient is w.r.t., and the grad w.r.t. the output. It should return a tensor pointer, where the tensor contains the gradient computation. Implement the function <code>sub</code> in <code>subop.cpp</code>. <code>sub</code> must work with and be compiled for <code>int</code>, <code>float</code>, and <code>double</code>. <code>sub</code> is also expected to work for all memory types. See <a href="#constructor">constructor</a>, <a href="#eval">eval</a>, <a href="#grad">grad</a>, <a href="#to_string">to_string</a>, and <a href="#func">func</a> for more information on how to implement these.</em></li>
<li><em>Add <code>#include "sub/subop.h"</code> to <code><a class="el" href="tensor__operations_8h_source.html">include/compute/tensor_operations.h</a></code>. This allows the rest of the library to see the new operation.</em></li>
<li><em>_Optional:</em> Add a tester file to the <code>testing/</code> folder.</li>
</ol>
<p>See <a href="/include/compute/add">include/compute/add/</a> and <a href="/src/compute/add">src/compute/add/</a> for an actual example.</p>
<p>Operators <em>can</em> implement copy and no-copy options, determining whether to return a newly allocated tensor or write over one of the parameters. However, this is not required.</p>
<p>The constructor must call the parent constructor of <code>Operation&lt;T&gt;</code> that takes a vector of operations. This sets the children of the operation and is used for memory releasing. <code>output_shape</code> and <code>mem_type</code> should also be set within the constructor. This allows shape checking and pre-allocation of tensors when the tree is created.</p>
<p>The constructor should also do any preprocessing that is possible, to remove computational burden from the performance critical <code>eval</code> function.</p>
<p>An example of a constructor might look like,</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">/* x is the only child here, so pass that to the parent class constructor. */</div><div class="line">template &lt;typename T&gt;</div><div class="line">TanhOp&lt;T&gt;::TanhOp(Operation&lt;T&gt; *x, bool needs_grad) : Operation&lt;T&gt;::Operation({x}, needs_grad), x(x) {</div><div class="line"></div><div class="line">    /* set the output shape and memory type */</div><div class="line">    this-&gt;output_shape = x-&gt;get_output_shape();</div><div class="line">    this-&gt;mem_type = x-&gt;get_memory_type();</div><div class="line"></div><div class="line">    /* create return tensor here to avoid memory allocation at tree execution */</div><div class="line">    this-&gt;output_tensor = new Tensor&lt;T&gt; (this-&gt;output_shape, this-&gt;mem_type);</div><div class="line">}</div></div><!-- fragment --><p>The eval method is simply responsible for the evaluation of the operation. It should return a tensor pointer with the same shape and memory type as defined by the operation. An example eval function might look like,</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">template &lt;typename T&gt;</div><div class="line">Tensor&lt;T&gt;* MatmulOp&lt;T&gt;::_eval(bool recompute) {</div><div class="line">    /* evaluate the child nodes */</div><div class="line">    a_tensor = a-&gt;eval(recompute);    // MxK</div><div class="line">    b_tensor = b-&gt;eval(recompute);    // KxN</div><div class="line">    c_tensor = c-&gt;eval(recompute);    // MxN</div><div class="line"></div><div class="line">    /* use an external utility function to multiply the matrices */</div><div class="line">    /* or use the preferred math::matmul(.) function */</div><div class="line">    internal::gemm_full(alpha, a_tensor, b_tensor, beta, this-&gt;output_tensor);</div><div class="line"></div><div class="line">    return this-&gt;output_tensor;</div><div class="line">} </div></div><!-- fragment --><p>Grad functions should write into the gradient tensor and return it. Every operation has a <code>std::map&lt;Operation&lt;T&gt;*, Tensor&lt;T&gt;*&gt; _grad_cache</code>, which can help store these tensors keyed on input variables. For instance, the <code>_grad</code> implementation of a log operation: </p><div class="fragment"><div class="line">{c++}</div><div class="line">template &lt;typename T&gt;</div><div class="line">Tensor&lt;T&gt; *LogOp&lt;T&gt;::_grad(Operation&lt;T&gt; *consumer, Operation&lt;T&gt; *var, Tensor&lt;T&gt; *grad) {</div><div class="line">    /* TODO : grad * (1/x) */</div><div class="line">    Tensor&lt;T&gt; *out;</div><div class="line"></div><div class="line">    this-&gt;x_tensor = x-&gt;eval(false);    /* don&#39;t recompute x if we don&#39;t have to */</div><div class="line"></div><div class="line">    out = this-&gt;_grad_cache[(uintptr_t)var];</div><div class="line">    if (out == NULL) {</div><div class="line">        out = new Tensor&lt;T&gt; (this-&gt;output_shape, {NONE,{}}, this-&gt;mem_type);</div><div class="line">        this-&gt;_grad_cache[(uintptr_t)var] = out;</div><div class="line">    }</div><div class="line"></div><div class="line">    internal::log_grad(x_tensor, grad, out);</div><div class="line"></div><div class="line">    return out;</div><div class="line">}</div></div><!-- fragment --><p>The <code>to_string</code> method is fairly simple to implement. It defines a form to print out the operation, using the <code>to_string</code> return values of the child operations. For instance, the operation <code>add(a,b)</code>'s implementation might look like</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">template &lt;typename T&gt;</div><div class="line">std::string AddOp&lt;T&gt;::to_string() {</div><div class="line">    return &quot;(&quot; + a-&gt;to_string() + &quot; + &quot; + b-&gt;to_string() + &quot;)&quot;;</div><div class="line">    /* OR something like this */</div><div class="line">    return &quot;ADD(&quot; + a-&gt;to_string() + &quot;, &quot; + b-&gt;to_string() + &quot;)&quot;;</div><div class="line">}</div></div><!-- fragment --><p>Every operation should be paired with a function that returns a new pointer to that operation. This allows for cleaner math expressions for the library user (i.e. <code>auto x = op::add(op::matmul(A, x),c)</code>). An example of this function might look like</p>
<div class="fragment"><div class="line">{c++}</div><div class="line">template &lt;typename T&gt;</div><div class="line">TanhOp&lt;T&gt;* tanh(Operation&lt;T&gt; *x, bool use_gpu) {</div><div class="line">    return new TanhOp&lt;T&gt; (x, use_gpu);</div><div class="line">}</div></div><!-- fragment --> </div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
